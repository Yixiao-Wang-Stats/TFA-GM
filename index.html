<!DOCTYPE html>
<html>
<meta property='og:title' content='Real-Time Video Generation with Pyramid Attention Broadcast'/>
<meta property='og:description' content='Real-Time Video Generation with Pyramid Attention Broadcast'/>
<meta property='og:url' content='https://oahzxl.github.io/PAB/'/>
<meta property='og:image:width' content='1200' />
<meta property='og:image:height' content='663' />
<!-- TYPE BELOW IS PROBABLY: 'website' or 'article' or look on https://ogp.me/#types -->
<meta property="og:type" content='website'/>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Real-Time Video Generation with Pyramid Attention Broadcast">
  <meta name="keywords" content="Real-Time Video Generation with Pyramid Attention Broadcast">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Real-Time Video Generation with Pyramid Attention Broadcast</title>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/tab_gallery.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/magnifier.js"></script>
  <script src="./static/js/particles.js"></script>
  <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/image_card_fader.css">
  <link rel="stylesheet" href="./static/css/image_card_slider.css">
  <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Patrick+Hand|Google+Sans|Noto+Sans|Castoro|Lato|Open+Sans&effect=shadow-multiple|emboss|3d"> 
  <link rel="icon" href="./static/images/pyramid.png" type="image/x-icon">
  <link rel="shortcut icon" href="./static/images/pyramid.png" type="image/x-icon">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<style>
  @import url('https://fonts.cdnfonts.com/css/menlo');

  .video-table td, .video-table th {
    padding-top: 2px;
    padding-bottom: 2px;
    padding-left: 4px;
    padding-right: 4px;
    font-weight: normal;
  }
  .first-col {
    width: 7%;
    vertical-align: middle;
  }
  .other-col {
    width: 31%;
  }
  body {
    font-family: "Lato", sans-serif;
    font-size: 1.1em;
  }
  .title.is-3 {
    font-weight: 900;
    font-size: 2.0rem;
  }
  .title.is-4 {
    font-weight: 700;
    font-size: 1.7rem;
  }
  .custom-emoji {
    width: 1em;
    height: 1em;
    display: inline-block;
    background-image: url('./static/images/pyramid.png');
    background-size: cover;
    vertical-align: middle;
    line-height: 1;
  }
   .star {
    position: absolute;
    pointer-events: none;
    z-index: 9999;
    will-change: transform, opacity;
  }

</style>


<body>
  <div id="particles-js" style="position: fixed; width: 100%; height: 100%; z-index: -1;"></div>
  <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <br><br>
          <h1 class="title is-2 publication-title" style="font-size: 2.12rem">
            <span style="color: #ff0000;">O</span><span style="color: #f80007;">n</span>&nbsp;
            <span style="color: #f1000e;">T</span><span style="color: #ea0015;">r</span><span style="color: #e4001c;">a</span><span style="color: #dd0022;">i</span><span style="color: #d60029;">n</span><span style="color: #cf0030;">i</span><span style="color: #c80037;">n</span><span style="color: #c1003e;">g</span><span style="color: #ba0045;">-</span>
            <span style="color: #b3004c;">F</span><span style="color: #ad0053;">r</span><span style="color: #a60059;">e</span><span style="color: #9f0060;">e</span>&nbsp;
            <span style="color: #980067;">A</span><span style="color: #91006e;">c</span><span style="color: #8a0075;">c</span><span style="color: #83007c;">e</span><span style="color: #7c0083;">l</span><span style="color: #76008a;">e</span><span style="color: #6f0090;">r</span><span style="color: #680097;">a</span><span style="color: #61009e;">t</span><span style="color: #5a00a5;">i</span><span style="color: #5400ac;">o</span><span style="color: #4d00b3;">n</span>&nbsp;
            of Generative Modeling
          </h1>

        
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/Ting-Justin-Jiang" target="_blank">Ting Jiang</a><sup>1*</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://yixiao-wang-stats.github.io/" target="_blank">Yixiao Wang</a><sup>1*</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://zishan-shao.github.io/" target="_blank">Zishan Shao</a><sup>1*</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://github.com/HankYe" target="_blank">Hancheng Ye</a><sup>1</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://ece.duke.edu/people/yiran-chen/" target="_blank">Yiran Chen</a><sup>1</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://ece.duke.edu/people/hai-helen-li/" target="_blank">Hai Li</a><sup>1</sup>
            </span>
          </div>


          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Duke University&nbsp;</span>
          </div>

          <div class="is-size-5 publication-authors">
            justin.jiang, yixiao.wang, zishan.shao, hancheng.ye, yiran.chen, hai.li@duke.edu
          </div>


          <div class="is-size-5 publication-authors">
            (* indicates equal contribution)
          </div>
           <div class="is-size-5 has-text-centered" style="margin-top: 0.5em; font-weight: bold;">
             &nbsp;
              🚀 <span style="
  background: linear-gradient(to right, #7b7bff, #cf77ef, #f3a3cd, #ffb68e);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  font-weight: bold;">
  SADA
</span> plugs straight into any project built on HuggingFace Diffusers 🤗
            </div>
          <!-- <div class="is-size-5 publication-venue">
            in XXX
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/NUS-HPC-AI-Lab/VideoSys" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- paper -->
              <span class="link-block">
                <a class="external-link button is-normal is-rounded is-light" disabled style="pointer-events: none; opacity: 0.6;">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Coming Soon)</span>
                </a>
              </span>
              
              <!-- doc -->
              <span class="link-block">
                <a class="external-link button is-normal is-rounded is-light" disabled style="pointer-events: none; opacity: 0.6;">
                  <span class="icon">
                    <i class="fas fa-book"></i>
                  </span>
                  <span>Doc (Coming Soon)</span>
                </a>
              </span>
              
              <!-- twitter -->
              <span class="link-block">
                <a class="external-link button is-normal is-rounded is-light" disabled style="pointer-events: none; opacity: 0.6;">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter (Coming Soon)</span>
                </a>
              </span>
              <!-- bibtex -->
              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-obp"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="content has-text-justified">
        <p>
          <b>Any speedup ratio without quality loss. Any modality. Any model. Any scheduler.</b>  
          Introducing 
          <span style="
            background: linear-gradient(to right, #7b7bff, #cf77ef, #f3a3cd, #ffb68e);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-weight: bold;">
            SADA
          </span> — a <b>training-free</b> acceleration method for generative models, running with <b>just one line of code</b>. Fully open-source.  
          Achieve <b>1.9×</b> <b>lossless acceleration</b> or <b>3.6×</b> speedup with LPIPS < 0.2 for <b>ultra-stable quality</b>.  
          Works seamlessly with <b>images</b>, <b>videos</b>, and <b>audio</b> across U-Net, DiT, and beyond — delivering <b>instant compatibility</b> and <b>breakthrough performance</b>.
        </p>
      </div>
    </div>
      <div class="has-text-centered" style="margin-top:2em;">
      <img src="static/images/SLlpips.png" alt="SADA Motivation Diagram" style="max-width:500px; width:65%;">
      <p style="font-size:0.95em; margin-top:0.5em;">
        <i><b>Figure 1.</b>  The purple line represents <span style="
            background: linear-gradient(to right, #7b7bff, #cf77ef, #f3a3cd, #ffb68e);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-weight: bold;">
            SADA
          </span>  results at different speed modes (Quality, Balanced, Medium, Fast, Turbo), <br>
      showing superior performance in both speedup and LPIPS compared to other methods.</i>
      </p>
    </div>
  </div>
</section>



<div class="scene" data-speed="1">
  <div class="row">
    <img src="static/images/boat/1.png">
    <img src="static/images/car/1.png">
    <img src="static/images/girl/1.png">
    <img src="static/images/sheep/1.png">
  </div>
  <p>Original (1.0x)</p>
</div>

<div class="scene" data-speed="1.92">
  <div class="row">
    <img src="static/images/boat/2.png">
    <img src="static/images/car/2.png">
    <img src="static/images/girl/2.png">
    <img src="static/images/sheep/2.png">
  </div>
  <p>Balanced (1.92x)</p>
</div>

<div class="scene" data-speed="2.25">
  <div class="row">
    <img src="static/images/boat/3.png">
    <img src="static/images/car/3.png">
    <img src="static/images/girl/3.png">
    <img src="static/images/sheep/3.png">
  </div>
  <p>Medium (2.25x)</p>
</div>

<div class="scene" data-speed="2.91">
  <div class="row">
    <img src="static/images/boat/4.png">
    <img src="static/images/car/4.png">
    <img src="static/images/girl/4.png">
    <img src="static/images/sheep/4.png">
  </div>
  <p>Fast (2.91x)</p>
</div>

<div class="scene" data-speed="3.64">
  <div class="row">
    <img src="static/images/boat/5.png">
    <img src="static/images/car/5.png">
    <img src="static/images/girl/5.png">
    <img src="static/images/sheep/5.png">
  </div>
  <p>Turbo (3.64x)</p>
</div>

<style>
.scene {
  margin-bottom: 20px;
  text-align: center;
}
.row {
  display: flex;
  gap: 10px;
  justify-content: center;
}
.row img {
  width: 150px;
  opacity: 0;
  transition: opacity 0.5s ease-in;
  border-radius: 5px;
  box-shadow: 0 4px 10px rgba(0,0,0,0.1);
}
.scene p {
  font-weight: bold;
  margin-top: 8px;
  font-size: 1em;
}
</style>


<script>
function startAnimation() {
  document.querySelectorAll('.scene').forEach(scene => {
    let speed = parseFloat(scene.dataset.speed);
    let imgs = scene.querySelectorAll('img');

    // reset opacity before animation
    imgs.forEach(img => img.style.opacity = 0);

    imgs.forEach((img, i) => {
      setTimeout(() => {
        img.style.opacity = 1;
      }, i * (1000 / speed));
    });
  });
}

let observer = new IntersectionObserver((entries, obs) => {
  entries.forEach(entry => {
    if (entry.isIntersecting) {
      startAnimation();
      obs.disconnect(); // 只执行一次
    }
  });
}, { threshold: 0.2 });

// 改成监听第一个 .scene，而不是不存在的 #animation-section
observer.observe(document.querySelector('.scene'));
</script>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Pyramid Attention Broadcast</h2>
        
        <h2 class="title is-4">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Recently, Sora and other DiT-based video generation models have attracted significant attention. However, in contrast to image generation, there are few studies focused on accelerating the inference of DiT-based video generation models. Additionally, the inference cost for generating a single video can be substantial, often requiring tens of GPU minutes or even hours. Therefore, accelerating the inference of video generation models has become urgent for broader GenAI applications.
          </p>
          <div class="content has-text-centered">
            <img src="./static/images/motivation.png" style="width: 50%;"><br>
            <span style="font-size: 0.8em; width: 75%; display: inline-block;">Figure 1: We compare the attention outputs differences between the current and previous diffusion steps. Differences are quantified using Mean Square Error (MSE) and averaged across all layers for each diffusion step.</span>
          </div>
        </div>

        <h2 class="title is-4">Implementation</h2>
        <div class="content has-text-justified">
          <p>
            Our study reveals two key observations of attention mechanisms in video diffusion transformers: Firstly, attention differences across time steps exhibit a U-shaped pattern, with significant variations occurring during the first and last 15% of steps, while the middle 70% of steps are very stable with minor differences. Secondly, within the stable middle segment, the differences varies among attention types: spatial attention varies the most, involving high-frequency elements like edges and textures; temporal attention exhibits mid-frequency variations related to movements and dynamics in videos; cross-modal attention is the most stable, linking text with video content, analogous to low-frequency signals reflecting textual semantics.
          </p>
          <div class="content has-text-centered">
            <img src="./static/images/method.png"><br>
            <span style="font-size: 0.8em; width: 100%; display: inline-block;">Figure 2: We propose pyramid attention broadcast (shown on the right side) which sets different broadcast ranges for three attentions based on their differences. The smaller the variation in attention, the broader the broadcast range. During runtime, we broadcast attention results to the next several steps (shown on the left side) to avoid redundant attention computations. \( x_t \) refers to the features at timestep \( t \).</span>
          </div>
          <div class="content has-text-justified">
            Building on these insights, we propose <span class="custom-emoji"></span><b><span style="color: #ff0000;">P</span><span style="color: #f80007;">y</span><span style="color: #f1000e;">r</span><span style="color: #ea0015;">a</span><span style="color: #e4001c;">m</span><span style="color: #dd0022;">i</span><span style="color: #d60029;">d</span> <span style="color: #cf0030;">A</span><span style="color: #c80037;">t</span><span style="color: #c1003e;">t</span><span style="color: #ba0045;">e</span><span style="color: #b3004c;">n</span><span style="color: #ad0053;">t</span><span style="color: #a60059;">i</span><span style="color: #9f0060;">o</span><span style="color: #980067;">n</span> <span style="color: #91006e;">B</span><span style="color: #8a0075;">r</span><span style="color: #83007c;">o</span><span style="color: #7c0083;">a</span><span style="color: #76008a;">d</span><span style="color: #6f0090;">c</span><span style="color: #680097;">a</span><span style="color: #61009e;">s</span><span style="color: #5a00a5;">t</span></b> to alleviate unnecessary attention computations. In the middle segment, where attentions show minor differences, we can broadcast one diffusion step's attention outputs to several subsequent steps, thereby significantly reducing computational costs. Furthermore, for more efficient computation and minimum quality loss, we set varied broadcast ranges for different attentions based on their stability and differences. This simple yet effective strategy achieves up to a 35% speedup with negligible quality loss, even without post-training.
          </div>
        </div>

        <h2 class="title is-4">Parallelism</h2>
        <div class="content has-text-centered">
          <div class="content has-text-centered">
            <img src="./static/images/parallel.png" style="width: 70%;"><br>
            <span style="font-size: 0.8em; width: 80%; display: inline-block;">Figure 3: Comparison between original Dynamic Sequence Parallel (DSP) and ours. When temporal attention is broadcasted, we can avoid all communication.</span>
          </div>
          <div class="content has-text-justified">
            To further enhance video generation speed, we improve sequence parallel based on <a href="https://arxiv.org/abs/2403.10266" target="_blank">Dynamic Sequence Parallelism</a> (DSP). Sequence parallelism segments videos into different parts across multiple GPUs, reducing the workload on each GPU and decreasing generation latency. However, DSP introduces significant communication overhead, requiring two all-to-all communications for temporal attention. By broadcasting temporal attention in <span style="color: #e2001d;">P</span><span style="color: #a80057;">A</span><span style="color: #6d0092;">B</span>, we eliminate these communications, as temporal attention no longer needs to be computed. This results in a significant reduction in communication overhead by over 50%, enabling more efficient distributed inference for real-time video generation.
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Evaluations</h2>
        
        <h2 class="title is-4">Speedups</h2>
        <div class="content has-text-centered">
          <img src="./static/images/speedup.png">
        </div>
        <div class="content has-text-justified">
          <p>
            Measured total latency of <span style="color: #e2001d;">P</span><span style="color: #a80057;">A</span><span style="color: #6d0092;">B</span> for different models for generating a single video on 8 NVIDIA H100 GPUs. When utilizing a single GPU, we achieve a speedup ranging from 1.26x to 1.32x, which remains stable across different schedulers. Scaling to multiple GPUs, our method achieves a speedup of up to 10.6x, which almost linearly scales with the number of GPUs due to our efficient improvement of sequence parallel.
          </p>
        </div>
      </div>
    </div>

    <h2 class="title is-4">Qualitative Results</h2>
    <div class="container is-max-desktop">
      <video class="video" autoplay controls muted loop playsinline>
        <source src="./static/videos/video1.mp4" type="video/mp4">
      </video>
      <video class="video" autoplay controls muted loop playsinline>
        <source src="./static/videos/video2.mp4" type="video/mp4">
      </video>
      <video class="video" autoplay controls muted loop playsinline>
        <source src="./static/videos/video3.mp4" type="video/mp4">
      </video>
      <br>
    </div>
  </div>
  <br>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">        
        <div class="content has-text-justified">
          <h2 class="title is-4">Quantitive Results</h2>
          <div class="content has-text-centered">
            <img src="./static/images/eval.png" style="width: 40%;"><br>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related works</h2>
        <div class="content has-text-justified">
          <p>
            <li>
              Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. <a href="https://github.com/hpcaitech/Open-Sora">Open-Sora: Democratizing Efficient Video Production for All</a>. GitHub, 2024.
            </li>
            <li>
              PKU-Yuan Lab and Tuzhan AI etc. <a href="https://github.com/PKU-YuanGroup/Open-Sora-Plan">Open-Sora-Plan</a>. GitHub, 2024.
            </li>
            <li>
              Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. <a href="https://arxiv.org/abs/2401.03048">Latte: Latent Diffusion Transformer for Video Generation</a>. arXiv, 2024.
            </li>
            <li>
              Xuanlei Zhao, Shenggan Cheng, Chang Chen, Zangwei Zheng, Ziming Liu, Zheming Yang, and Yang You. <a href="https://arxiv.org/abs/2401.03048">DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers</a>. arXiv, 2024.
            </li>
            <li>
              Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, and Song Han. <a href="https://arxiv.org/abs/2402.19481">DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</a>. In CVPR, 2024.
            </li>
            <li>
              Xinyin Ma, Gongfan Fang, and Xinchao Wang. <a href="https://arxiv.org/abs/2312.00858">DeepCache: Accelerating Diffusion Models for Free</a>. In CVPR, 2024.
            </li>
            <li>
              Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. <a href="https://arxiv.org/abs/2311.17982">VBench: Comprehensive Benchmark Suite for Video Generative Models</a>. In CVPR, 2024.
            </li>
            <li>
              David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. <a href="https://arxiv.org/abs/2406.11816">Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation</a>. arXiv, 2024.
            </li>
            <li>
              Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. <a href="https://arxiv.org/abs/2209.14792">Make-A-Video: Text-to-Video Generation without Text-Video Data</a>. In ICLR, 2023.
            </li>
            <li>
              Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. <a href="https://arxiv.org/abs/2401.12945">Lumiere: A Space-Time Diffusion Model for Video Generation</a>. arXiv, 2024.
            </li>
            <li>
              Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. <a href="https://arxiv.org/abs/2304.08818">Align your Latents:
                High-Resolution Video Synthesis with Latent Diffusion Models</a>. In CVPR, 2023.
            </li>
            <li>
              Zhengqing Yuan, Ruoxi Chen, Zhaoxu Li, Haolong Jia, Lifang He, Chi Wang, and Lichao Sun. <a href="https://arxiv.org/abs/2403.13248">Mora: Enabling Generalist Video Generation via A Multi-Agent Framework</a>. In CVPR, 2023.
            </li>
          </p>
        </div>

        <h2 class="title is-3">Acknowledgments</h2>
        <div class="content has-text-justified">
          <p>
            Xuanlei, Xiaolong, and Kai contribute equally to this work. Kai and Yang are equal advising.
          </p>
        </div>
      </div>
    </div>
  </div>

</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>@misc{zhao2024pab,
      title={Real-Time Video Generation with Pyramid Attention Broadcast},
      author={Xuanlei Zhao and Xiaolong Jin and Kai Wang and Yang You},
      year={2024},
      eprint={2408.12588},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.12588},
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    
<script>
  let x1=0, y1=0;
  const 
    dist_to_draw = 20,
    delay = 1000,
    fsize = ['1.1rem', '1.4rem', '.8rem', '1.7rem'],
    colors = ['#E23636', '#F9F3EE', '#E1F8DC', '#B8AFE6', '#AEE1CD', '#5EB0E5'],
    rand = (min, max) => Math.floor(Math.random() * (max - min + 1)) + min,
    selRand = (o) => o[rand(0, o.length -1)],
    distanceTo =  (x1, y1, x2, y2) => Math.sqrt((Math.pow(x2-x1,2))+(Math.pow(y2-y1,2))),
    shouldDraw = (x, y) => (distanceTo(x1, y1, x, y) >= dist_to_draw),
    addStr = (x, y) => {
      const str = document.createElement("div");
      str.innerHTML = '&#10022;';
      str.className = 'star';
      str.style.top = `${y + rand(-20,20)}px`;
      str.style.left = `${x}px`;
      str.style.color = selRand(colors);
      str.style.fontSize = selRand(fsize);
      document.body.appendChild(str);
      const fs = 10 + 5 * parseFloat(getComputedStyle(str).fontSize);
      str.animate({
        translate: `0 ${fs}px`,
        opacity: 0,
        transform: `rotateX(${rand(1, 500)}deg) rotateY(${rand(1, 500)}deg)`
      }, {
        duration: delay,
        fill: 'forwards',
      });
      setTimeout(() => str.remove(), delay);
    };

  addEventListener("mousemove", (e) => {
    const { pageX, pageY } = e;
    if (shouldDraw(pageX, pageY)) {
      addStr(pageX, pageY);
      x1 = pageX;
      y1 = pageY;
    }
  });
</script>

</body>
</html>
